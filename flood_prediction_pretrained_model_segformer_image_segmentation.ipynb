{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tPG2J1YIqYKg",
    "outputId": "6400f292-ff65-4010-c85d-426762247b0c"
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets torch torchvision tifffile pillow\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tifffile as tiff\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoModelForSemanticSegmentation\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/data-20250917T185435Z-1-001/data\"\n",
    "IMAGE_DIR = os.path.join(BASE_DIR, \"images\")\n",
    "MASK_DIR  = os.path.join(BASE_DIR, \"labels\")\n",
    "\n",
    "def load_image(path):\n",
    "    img = tiff.imread(path).astype(np.float32)\n",
    "    img = img[:, :, [3,2,1]]\n",
    "    img = img / np.max(img)\n",
    "    return img\n",
    "\n",
    "def load_mask(path):\n",
    "    mask = np.array(Image.open(path))\n",
    "    mask = (mask > 0).astype(np.uint8)\n",
    "    return mask\n",
    "\n",
    "\n",
    "image_files = sorted([f for f in os.listdir(IMAGE_DIR) if f.endswith(\".tif\")])\n",
    "mask_files  = sorted(os.listdir(MASK_DIR))\n",
    "\n",
    "image_paths, mask_paths = [], []\n",
    "for img_name in image_files:\n",
    "    base = os.path.splitext(img_name)[0]\n",
    "    for ext in [\".png\", \".jpg\", \".tif\"]:\n",
    "        mask_name = base + ext\n",
    "        if mask_name in mask_files:\n",
    "            image_paths.append(os.path.join(IMAGE_DIR, img_name))\n",
    "            mask_paths.append(os.path.join(MASK_DIR, mask_name))\n",
    "            break\n",
    "\n",
    "print(f\"âœ… Matched pairs: {len(image_paths)}\")\n",
    "\n",
    "\n",
    "class WaterSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = load_image(self.image_paths[idx])\n",
    "        mask = load_mask(self.mask_paths[idx])\n",
    "\n",
    "        img = torch.tensor(img).permute(2,0,1).float()\n",
    "        mask = torch.tensor(mask).long()\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "train_imgs, val_imgs, train_masks, val_masks = train_test_split(\n",
    "    image_paths, mask_paths, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_dataset = WaterSegmentationDataset(train_imgs, train_masks)\n",
    "val_dataset   = WaterSegmentationDataset(val_imgs, val_masks)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "2a3157cb969040239dd474d874445f3e",
      "ec3908ac699e4867954a946eefd56f84",
      "b4be8ab9b1354cfd9dc07af31bda7a88",
      "caac1f00a5ce4ca5b7379bbe9c798a03",
      "2cdf463c08bc4a05930a4ec1af3cc2f0",
      "1b9de2e5140441b1875bafcbaf03748c",
      "d2a7e031815c49768bcdd37fa463e7b2",
      "5c013193f48547eda3fa8d36cc724a76",
      "ffeff74e834a4d45b620aa1de4316f19",
      "13ddf88707954f5a97bde7a8df90f05f",
      "d617641d460b47cd98aa09dc37610ba8",
      "3c0ed5cd429347719272027661735a9f",
      "c54169171b8e4474a3c1ef0bf51a0950",
      "1029c44f248c44d696ba3c67b18703c3",
      "f18c62dfa6f44e6ea7e1e2176a417778",
      "63c61f00f2d343fa9834d8be4494457b",
      "c67295e39cf946ca91437d27d622148c",
      "fd0c0dfb38a44f4e9f32e8da7b6be127",
      "77c0f75aaefb4908b83011e35216a1e0",
      "d3af878c8de7420e965e1a7fcc85f3c5",
      "7320002190024f739342cd548ea534c8",
      "836db3c9094e40e891101650262f751e"
     ]
    },
    "id": "f-1ZLS-W7Uku",
    "outputId": "8dfe73a8-ac5e-478b-e311-acb986c27e92"
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/segformer-b0-finetuned-ade-512-512\"\n",
    ")\n",
    "\n",
    "in_channels = model.decode_head.classifier.in_channels\n",
    "model.decode_head.classifier = nn.Conv2d(in_channels, 2, kernel_size=1)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "def compute_iou(pred_mask, true_mask):\n",
    "    pred = pred_mask.cpu().numpy()\n",
    "    true = true_mask.cpu().numpy()\n",
    "    intersection = np.logical_and(true, pred).sum()\n",
    "    union = np.logical_or(true, pred).sum()\n",
    "    return intersection / (union + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XGsnsE-Q7VY-",
    "outputId": "9c6ae261-f747-4318-ee6d-0d7298e008ab"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, train_iou = 0, 0\n",
    "\n",
    "        for imgs, masks in train_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(pixel_values=imgs).logits\n",
    "\n",
    "            outputs = torch.nn.functional.interpolate(\n",
    "                outputs,\n",
    "                size=masks.shape[-2:],\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False\n",
    "            )\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            train_iou += compute_iou(preds, masks)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_iou = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for imgs, masks in val_loader:\n",
    "                imgs, masks = imgs.to(device), masks.to(device)\n",
    "                outputs = model(pixel_values=imgs).logits\n",
    "                outputs = torch.nn.functional.interpolate(\n",
    "                    outputs,\n",
    "                    size=masks.shape[-2:],\n",
    "                    mode=\"bilinear\",\n",
    "                    align_corners=False\n",
    "                )\n",
    "                val_loss += criterion(outputs, masks).item()\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_iou += compute_iou(preds, masks)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} \"\n",
    "              f\"- Train Loss: {train_loss/len(train_loader):.4f}, IoU: {train_iou/len(train_loader):.4f} \"\n",
    "              f\"- Val Loss: {val_loss/len(val_loader):.4f}, IoU: {val_iou/len(val_loader):.4f}\")\n",
    "\n",
    "train_model(model, train_loader, val_loader, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L3yYv62g7dgm"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def test_model(model, image_path, mask_path=None, threshold=0.5):\n",
    "    model.eval()\n",
    "    img = load_image(image_path)\n",
    "    X = torch.tensor(img).permute(2,0,1).unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(pixel_values=X).logits\n",
    "        pred = F.interpolate(pred, size=(img.shape[0], img.shape[1]), mode=\"bilinear\", align_corners=False)\n",
    "        pred_mask = torch.argmax(pred, dim=1).squeeze().cpu().numpy()\n",
    "\n",
    "    rgb_img = (img - img.min()) / (img.max() - img.min())\n",
    "\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(rgb_img)\n",
    "    plt.title(\"Input (RGB)\")\n",
    "\n",
    "    if mask_path:\n",
    "        gt_mask = load_mask(mask_path)\n",
    "        plt.subplot(1,3,2)\n",
    "        plt.imshow(gt_mask, cmap=\"gray\")\n",
    "        plt.title(\"Ground Truth\")\n",
    "\n",
    "        intersection = np.logical_and(gt_mask, pred_mask).sum()\n",
    "        union = np.logical_or(gt_mask, pred_mask).sum()\n",
    "        iou = intersection / (union + 1e-6)\n",
    "        print(f\"IoU: {iou:.4f}\")\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(pred_mask, cmap=\"gray\")\n",
    "    plt.title(\"Prediction\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "JqvpTeteDMOn",
    "outputId": "b2edc5b6-3acb-46ae-b35e-2a873ffd5f60"
   },
   "outputs": [],
   "source": [
    "test_model(model=model, image_path=\"/content/drive/MyDrive/data-20250917T185435Z-1-001/data/images/52.tif\", mask_path=\"/content/drive/MyDrive/data-20250917T185435Z-1-001/data/labels/52.png\", threshold=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBjm4xvFHwWg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
